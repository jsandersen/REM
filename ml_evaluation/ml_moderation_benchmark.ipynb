{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML experiment of our moderation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==3.5.1 sentence-transformers==0.3.9\n",
    "#!pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models as stm\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "nRowsRead = None\n",
    "df0 = pd.read_csv('labeled_data.csv', delimiter=',', nrows = nRowsRead)\n",
    "df0.dataframeName = 'labeled_data.csv'\n",
    "nRow, nCol = df0.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-structure\n",
    "c=df0['class']\n",
    "df0.rename(columns={'tweet' : 'text', 'class' : 'category'}, inplace=True)\n",
    "a=df0['text']\n",
    "b=df0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n",
    "\n",
    "df= pd.concat([a,b,c], axis=1)\n",
    "df.rename(columns={'class' : 'label'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary labelling task\n",
    "def mapper(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    if x == 1:\n",
    "        return 0\n",
    "    if x == 2:\n",
    "        return 1\n",
    "\n",
    "df['label'] = df['label'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked, valid = np.bincount(df['label'])\n",
    "total = blocked + valid\n",
    "print('Examples:\\n    Total: {}\\n    Valid (Neither): {} ({:.2f}% of total)\\n'.format(\n",
    "    total, valid, 100 * valid / total))\n",
    "print('Examples:\\n    Total: {}\\n    Blocked (Ofensive and Hate): {} ({:.2f}% of total)\\n'.format(\n",
    "    total, blocked, 100 * blocked / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test, y_train_, y_test = train_test_split(\n",
    "    df.index.values,\n",
    "    df.label.values,\n",
    "    test_size=0.365,\n",
    "    random_state=42,\n",
    "    stratify=df.label.values,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.loc[X_train_].index.values,\n",
    "    df.loc[X_train_].label.values,\n",
    "    test_size=0.50,\n",
    "    random_state=43,\n",
    "    stratify=df.loc[X_train_].label.values,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "df.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['category', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[df[\"data_type\"]==\"train\"]\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df.loc[df[\"data_type\"]==\"val\"]\n",
    "df_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.loc[df[\"data_type\"]==\"test\"]\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['text'].values\n",
    "y_train = df_train['label'].values\n",
    "\n",
    "X_val = df_val['text'].values\n",
    "y_val = df_val['label'].values\n",
    "\n",
    "X_test = df_test['text'].values\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = embedding_model.encode(X_train, show_progress_bar=True)\n",
    "X_val = embedding_model.encode(X_val, show_progress_bar=True)\n",
    "X_test = embedding_model.encode(X_test, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(768))\n",
    "X = Dropout(0.5)(input, training=True)\n",
    "X = Dense(1000, activation='relu')(X)\n",
    "X = Dropout(0.5)(X, training=True)\n",
    "X = Dense(2, activation='softmax')(X)\n",
    "model = Model(inputs=input, outputs = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, batch_size=16, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#model = tensorflow.keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, x, no_classes=2, n_iter=50):\n",
    "    result = np.zeros((n_iter,) + (x.shape[0], no_classes))\n",
    "    for i in range(n_iter):\n",
    "        result[i,:, :] = model.predict(x)\n",
    "\n",
    "    prediction = result.mean(axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_with_uncertainty(model, X_test)\n",
    "y_pred = prediction.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced accuracy\n",
    "balanced_accuracy_score(y_test.argmax(axis=1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score\n",
    "f1_score(y_test.argmax(axis=1), y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df\n",
    "data = {'y_pred': y_pred, 'y_true': y_test.argmax(axis=1), 'unc': 1 - prediction.max(axis=1), }\n",
    "df = pd.DataFrame(data=data)\n",
    "df['def'] = df['y_pred'] != df['y_true']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by uncertainty\n",
    "df_sort = df.sort_values('unc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc confusion_matrix\n",
    "pred_def = df_sort['def'].values\n",
    "y_true = df_sort['y_true'].values\n",
    "y_pred = df_sort['y_pred'].values\n",
    "cm = confusion_matrix(y_true, y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion_matrix\n",
    "labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "categories = ['Zero', 'One']\n",
    "\n",
    "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
    "\n",
    "sns.set(font_scale=1.6) # for label size\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2%', cmap='Blues', \n",
    "           xticklabels=['Valid', 'Blocked'],\n",
    "           yticklabels=['Valid', 'Blocked'],\n",
    "           cbar=False\n",
    "          ) # font size\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc confusion_matrix with manual moderation\n",
    "mod = 1809\n",
    "balanced_accuracy_score(y_true, np.concatenate((y_pred[:len(y_true)-mod], y_true[len(y_true)-mod:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, np.concatenate((y_pred[:len(y_true)-mod], y_true[len(y_true)-mod:])), normalize='true')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion_matrix with manual moderation\n",
    "array = cm\n",
    "\n",
    "labels = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "categories = ['Zero', 'One']\n",
    "\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "\n",
    "sns.set(font_scale=1.6) # for label size\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2%', cmap='Blues', \n",
    "           xticklabels=['Blocked', 'Valid'],\n",
    "           yticklabels=['Blocked', 'Valid'],\n",
    "           cbar=False\n",
    "          ) # font size\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc \n",
    "mod_effort = []\n",
    "\n",
    "for i in tqdm(range(len(y_test)+1)):\n",
    "    ai = y_pred[:len(y_test)-i]\n",
    "    human = y_true[len(ai):]\n",
    "    \n",
    "    mod_y = np.concatenate((ai, human))\n",
    "    f1 = balanced_accuracy_score(y_true, mod_y)\n",
    "    mod_effort.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development of the balanced accuracy\n",
    "fontsize = 20\n",
    "\n",
    "plt.plot(mod_effort, label='Uncertainty')\n",
    "plt.plot([0, len(mod_effort)], [mod_effort[0], 1], 'black', linestyle='dashed', label='Random')\n",
    "plt.xlim((0, len(mod_effort)))\n",
    "plt.xticks(np.arange(0, 9047+1809.4, 1809.4), fontsize=fontsize)\n",
    "plt.yticks(np.arange(0.8, 1.05, 0.05), fontsize=fontsize)\n",
    "plt.ylabel('Balanced Accuracy', fontsize=fontsize)\n",
    "plt.xlabel('Moderation Effort', fontsize=fontsize)\n",
    "plt.xticks(np.arange(0, 9047+1809.4, 1809.4), ['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "plt.ylim((mod_effort[0], 1.0005))\n",
    "l = plt.legend(frameon=True, fontsize=fontsize, title=\"Moderation Strategy\", fancybox=True)\n",
    "plt.setp(l.get_title(),fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
